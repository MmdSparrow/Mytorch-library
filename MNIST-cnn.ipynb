{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"If it does not work use code below while running this notebook in this dir.\"\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath(os.getcwd()))\n",
    "\n",
    "from mytorch import Tensor, Model\n",
    "from mytorch import activation as active_func\n",
    "from mytorch import loss as loss_func\n",
    "from mytorch import optimizer as optim \n",
    "from mytorch import layer as nn\n",
    "from mytorch.util import DataLoader\n",
    "\n",
    "from mytorch.util import flatten\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load data set with given data loader.\n",
    "you have 10000 train data, 1000 for each number, remember to shuffle training data.\n",
    "you have 1000 test data, 100 for each number.\n",
    "\n",
    "loaded data is a list of (img, label)\n",
    "type of img is Tensor.\n",
    "\n",
    "TODO: you have to get this list and create batches for training.\n",
    "you can also apply this changes later in the Training part for convenience.\n",
    "\"\"\"\n",
    "data_loader = DataLoader(train_addr='MNIST/train', test_addr='MNIST/test')\n",
    "data_loader.load(train_batch_size=128, test_batch_size=32, do_normalize=True)\n",
    "\"you can see how data is loaded\"\n",
    "# print(data_loader.getTrain()[0][0].shape)\n",
    "# print(data_loader.getTrain()[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Create your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        \"TODO: define your layers. order is important\" \n",
    "        self.conv2d1= nn.Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
    "        self.conv2d2= nn.Conv2d(10, 10, kernel_size=(5, 5), stride=(1, 1))\n",
    "        self.mp = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.fc1 = nn.Linear(4*4*10,100, need_bias=False)\n",
    "        self.fc2 = nn.Linear(100,10, need_bias=False)\n",
    "        self.relu = active_func.relu\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"TODO: define forward pass\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x) #24x24x10\n",
    "        x = self.mp(x) #12x12x10\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x) #8x8x10\n",
    "        x = self.mp(x) #4x4x10    \n",
    "        # x = x.view(-1, 4*4*10) #flattening\n",
    "        x = flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu()\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"TODO: choose a proper loss function\"\n",
    "criterion = loss_func.CategoricalCrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"TODO: choose a proper optimizer\"\n",
    "optimizer = optim.SGD(model.parameters(), learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just use it for argmax\n",
    "import numpy as np\n",
    "\n",
    "def train_one_epoch(model: Model, data_loader, optimizer: optim, criterion, length):\n",
    "  acc = 0\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  for batch in data_loader.getTrain():\n",
    "      x_train_dev = batch[0]\n",
    "      y_train_dev = batch[1]\n",
    "\n",
    "      y_pred_dev = model(x_train_dev)\n",
    "\n",
    "      loss = criterion(y_pred_dev, y_train_dev)\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "      yp= np.argmax(y_pred_dev.data, axis=1)\n",
    "      acc += np.sum(yp == y_train_dev.data)\n",
    "\n",
    "  return acc/length\n",
    "\n",
    "\n",
    "\n",
    "def caluculate_acc(data_loader, model, length):\n",
    "  acc = 0\n",
    "  for batch in data_loader.getTest():\n",
    "    x_test_dev = batch[0]\n",
    "    y_test_dev = batch[1]\n",
    "    # show_batch(x_test_dev, y_test_dev)\n",
    "    yp = model(x_test_dev)\n",
    "    \n",
    "    yp= np.argmax(yp.data, axis=1)\n",
    "    acc += np.sum(yp == y_test_dev.data)\n",
    "  return acc / length\n",
    "\n",
    "\n",
    "def show_batch(X_train, y_train):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for i in range(64):\n",
    "      plt.subplot(8,8,i+1)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      plt.grid(False)\n",
    "      plt.imshow(X_train[i].data, cmap=plt.cm.binary)\n",
    "      plt.xlabel(y_train[i].data)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"TODO: define number of epoch and train batches of data on your model. also test each epoch.\"\n",
    "EPOCH = 10\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "best_train_acc = 0.0\n",
    "best_test_acc = 0.0\n",
    "\n",
    "for i in tqdm(range(EPOCH)):\n",
    "\n",
    "    \"TODO: train over your defined batches and save train accuracy for each epoch.\"\n",
    "    # train_acc.append(train_one_epoch(model, data_loader, optimizer, criterion, 10000)[1].item())\n",
    "    train_acc.append(train_one_epoch(model, data_loader, optimizer, criterion, 10000))\n",
    "    \n",
    "    if best_train_acc < train_acc[-1]:\n",
    "        best_train_acc = train_acc[-1]\n",
    "\n",
    "    \"TODO: test your model after each training and save test accuracy for each epoch.\"\n",
    "\n",
    "    # test_acc.append(caluculate_acc(data_loader, model, 1000).item())\n",
    "    test_acc.append(caluculate_acc(data_loader, model, 1000))\n",
    "\n",
    "\n",
    "    if best_test_acc < test_acc[-1]:\n",
    "        best_test_acc = test_acc[-1]\n",
    "\n",
    "    if train_acc[-1] > 0.99 and test_acc[-1] > 0.99:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc, label='train accuracy')\n",
    "plt.plot(test_acc, label = 'test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOn train - best accuracy: {:.2f}, final accuracy: {:.2f}\".format(best_train_acc, train_acc[-1]))\n",
    "print(\"On test - best accuracy: {:.2f}, final accuracy: {:.2f}\".format(best_test_acc, test_acc[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
