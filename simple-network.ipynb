{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"If it does not work use code below while running this notebook in this dir.\"\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath(os.getcwd()))\n",
    "\n",
    "from mytorch import Tensor\n",
    "from mytorch import activation as active_func\n",
    "from mytorch import loss as loss_func\n",
    "from mytorch import optimizer as optim \n",
    "from mytorch import layer as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {
    "simple network.drawio.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Network\n",
    "here you have to define a simple linear layer to find coefficients of x_i in below problem. <br>\n",
    "y = ax_1 + bx_2 + cx_3 <br><br>\n",
    "![simple network.drawio.png](<attachment:simple network.drawio.png>)\n",
    "<br><br>\n",
    "weights of this linear layer is your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"TODO: set coefficients\"\n",
    "a = 0.5\n",
    "b = 0.5\n",
    "c = 0.5\n",
    "\n",
    "X = Tensor(np.random.randn(100, 3))\n",
    "coef = Tensor(np.array([[a], [b], [c]]))\n",
    "y = X @ coef + 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial W and bias:\n",
      "Tensor([[-0.40685953]\n",
      " [-0.30351888]\n",
      " [ 0.77090916]], requires_grad=True)\n",
      "None\n",
      "layer.weight.data: [[-0.40685953]\n",
      " [-0.30351888]\n",
      " [ 0.77090916]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.00355563]\n",
      " [-0.00797063]\n",
      " [-0.01389484]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.4033039 ]\n",
      " [-0.29554825]\n",
      " [ 0.784804  ]]\n",
      "layer.weight.data: [[-0.4033039 ]\n",
      " [-0.29554825]\n",
      " [ 0.784804  ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.03862108]\n",
      " [-0.00825969]\n",
      " [ 0.00824107]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.36468282]\n",
      " [-0.28728856]\n",
      " [ 0.77656292]]\n",
      "layer.weight.data: [[-0.36468282]\n",
      " [-0.28728856]\n",
      " [ 0.77656292]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.00297365]\n",
      " [-0.00756439]\n",
      " [-0.01380772]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.36170917]\n",
      " [-0.27972418]\n",
      " [ 0.79037064]]\n",
      "layer.weight.data: [[-0.36170917]\n",
      " [-0.27972418]\n",
      " [ 0.79037064]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.03751476]\n",
      " [-0.00796011]\n",
      " [ 0.00832786]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.32419441]\n",
      " [-0.27176407]\n",
      " [ 0.78204278]]\n",
      "layer.weight.data: [[-0.32419441]\n",
      " [-0.27176407]\n",
      " [ 0.78204278]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.00241513]\n",
      " [-0.00717551]\n",
      " [-0.01372297]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.32177928]\n",
      " [-0.26458855]\n",
      " [ 0.79576575]]\n",
      "layer.weight.data: [[-0.32177928]\n",
      " [-0.26458855]\n",
      " [ 0.79576575]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.03645275]\n",
      " [-0.0076735 ]\n",
      " [ 0.00841205]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.28532654]\n",
      " [-0.25691505]\n",
      " [ 0.7873537 ]]\n",
      "layer.weight.data: [[-0.28532654]\n",
      " [-0.25691505]\n",
      " [ 0.7873537 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.00187911]\n",
      " [-0.00680327]\n",
      " [-0.01364053]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.28344743]\n",
      " [-0.25011179]\n",
      " [ 0.80099423]]\n",
      "layer.weight.data: [[-0.28344743]\n",
      " [-0.25011179]\n",
      " [ 0.80099423]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.03543327]\n",
      " [-0.00739931]\n",
      " [ 0.0084937 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.24801416]\n",
      " [-0.24271248]\n",
      " [ 0.79250053]]\n",
      "layer.weight.data: [[-0.24801416]\n",
      " [-0.24271248]\n",
      " [ 0.79250053]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.00136469]\n",
      " [-0.00644694]\n",
      " [-0.01356034]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.24664947]\n",
      " [-0.23626553]\n",
      " [ 0.80606088]]\n",
      "layer.weight.data: [[-0.24664947]\n",
      " [-0.23626553]\n",
      " [ 0.80606088]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.03445461]\n",
      " [-0.00713701]\n",
      " [ 0.00857288]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.21219485]\n",
      " [-0.22912852]\n",
      " [ 0.797488  ]]\n",
      "layer.weight.data: [[-0.21219485]\n",
      " [-0.22912852]\n",
      " [ 0.797488  ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.000871  ]\n",
      " [-0.00610586]\n",
      " [-0.01348237]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.21132385]\n",
      " [-0.22302265]\n",
      " [ 0.81097037]]\n",
      "layer.weight.data: [[-0.21132385]\n",
      " [-0.22302265]\n",
      " [ 0.81097037]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.03351514]\n",
      " [-0.00688609]\n",
      " [ 0.00864966]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.17780871]\n",
      " [-0.21613656]\n",
      " [ 0.80232071]]\n",
      "layer.weight.data: [[-0.17780871]\n",
      " [-0.21613656]\n",
      " [ 0.80232071]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.0003972 ]\n",
      " [-0.00577939]\n",
      " [-0.01340655]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.17741151]\n",
      " [-0.21035717]\n",
      " [ 0.81572726]]\n",
      "layer.weight.data: [[-0.17741151]\n",
      " [-0.21035717]\n",
      " [ 0.81572726]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.03261329]\n",
      " [-0.00664606]\n",
      " [ 0.00872411]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.14479822]\n",
      " [-0.20371111]\n",
      " [ 0.80700315]]\n",
      "layer.weight.data: [[-0.14479822]\n",
      " [-0.20371111]\n",
      " [ 0.80700315]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 5.75128155e-05]\n",
      " [-5.46688905e-03]\n",
      " [-1.33328483e-02]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.14485573]\n",
      " [-0.19824422]\n",
      " [ 0.82033599]]\n",
      "layer.weight.data: [[-0.14485573]\n",
      " [-0.19824422]\n",
      " [ 0.82033599]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.03174753]\n",
      " [-0.00641646]\n",
      " [ 0.0087963 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.1131082 ]\n",
      " [-0.19182777]\n",
      " [ 0.81153969]]\n",
      "layer.weight.data: [[-0.1131082 ]\n",
      " [-0.19182777]\n",
      " [ 0.81153969]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00049391]\n",
      " [-0.00516778]\n",
      " [-0.0132612 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.11360212]\n",
      " [-0.18665999]\n",
      " [ 0.8248009 ]]\n",
      "layer.weight.data: [[-0.11360212]\n",
      " [-0.18665999]\n",
      " [ 0.8248009 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.03091643]\n",
      " [-0.00619682]\n",
      " [ 0.00886628]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.08268569]\n",
      " [-0.18046316]\n",
      " [ 0.81593461]]\n",
      "layer.weight.data: [[-0.08268569]\n",
      " [-0.18046316]\n",
      " [ 0.81593461]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00091273]\n",
      " [-0.00488148]\n",
      " [-0.01319157]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.08359842]\n",
      " [-0.17558168]\n",
      " [ 0.82912618]]\n",
      "layer.weight.data: [[-0.08359842]\n",
      " [-0.17558168]\n",
      " [ 0.82912618]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.03011858]\n",
      " [-0.00598674]\n",
      " [ 0.00893413]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.05347984]\n",
      " [-0.16959494]\n",
      " [ 0.82019206]]\n",
      "layer.weight.data: [[-0.05347984]\n",
      " [-0.16959494]\n",
      " [ 0.82019206]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00131469]\n",
      " [-0.00460746]\n",
      " [-0.0131239 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.05479453]\n",
      " [-0.16498748]\n",
      " [ 0.83331595]]\n",
      "layer.weight.data: [[-0.05479453]\n",
      " [-0.16498748]\n",
      " [ 0.83331595]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02935267]\n",
      " [-0.00578579]\n",
      " [ 0.00899989]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.02544186]\n",
      " [-0.15920168]\n",
      " [ 0.82431607]]\n",
      "layer.weight.data: [[-0.02544186]\n",
      " [-0.15920168]\n",
      " [ 0.82431607]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00170046]\n",
      " [-0.00434519]\n",
      " [-0.01305814]], requires_grad=False)\n",
      "after update layer.weight.data: [[-0.02714232]\n",
      " [-0.15485649]\n",
      " [ 0.83737421]]\n",
      "layer.weight.data: [[-0.02714232]\n",
      " [-0.15485649]\n",
      " [ 0.83737421]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.0286174 ]\n",
      " [-0.00559359]\n",
      " [ 0.00906362]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.00147508]\n",
      " [-0.1492629 ]\n",
      " [ 0.82831059]]\n",
      "layer.weight.data: [[ 0.00147508]\n",
      " [-0.1492629 ]\n",
      " [ 0.82831059]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00207069]\n",
      " [-0.00409418]\n",
      " [-0.01299426]], requires_grad=False)\n",
      "after update layer.weight.data: [[-5.95611649e-04]\n",
      " [-1.45168718e-01]\n",
      " [ 8.41304847e-01]]\n",
      "layer.weight.data: [[-5.95611649e-04]\n",
      " [-1.45168718e-01]\n",
      " [ 8.41304847e-01]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02791154]\n",
      " [-0.00540976]\n",
      " [ 0.0091254 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.02731593]\n",
      " [-0.13975896]\n",
      " [ 0.83217945]]\n",
      "layer.weight.data: [[ 0.02731593]\n",
      " [-0.13975896]\n",
      " [ 0.83217945]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00242602]\n",
      " [-0.00385393]\n",
      " [-0.0129322 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.02488991]\n",
      " [-0.13590503]\n",
      " [ 0.84511165]]\n",
      "layer.weight.data: [[ 0.02488991]\n",
      " [-0.13590503]\n",
      " [ 0.84511165]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02723393]\n",
      " [-0.00523393]\n",
      " [ 0.00918527]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.05212384]\n",
      " [-0.13067109]\n",
      " [ 0.83592638]]\n",
      "layer.weight.data: [[ 0.05212384]\n",
      " [-0.13067109]\n",
      " [ 0.83592638]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00276705]\n",
      " [-0.003624  ]\n",
      " [-0.01287192]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.04935679]\n",
      " [-0.12704709]\n",
      " [ 0.8487983 ]]\n",
      "layer.weight.data: [[ 0.04935679]\n",
      " [-0.12704709]\n",
      " [ 0.8487983 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02658341]\n",
      " [-0.00506577]\n",
      " [ 0.00924328]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.0759402 ]\n",
      " [-0.12198132]\n",
      " [ 0.83955503]]\n",
      "layer.weight.data: [[ 0.0759402 ]\n",
      " [-0.12198132]\n",
      " [ 0.83955503]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00309434]\n",
      " [-0.00340395]\n",
      " [-0.01281338]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.07284586]\n",
      " [-0.11857737]\n",
      " [ 0.8523684 ]]\n",
      "layer.weight.data: [[ 0.07284586]\n",
      " [-0.11857737]\n",
      " [ 0.8523684 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02595891]\n",
      " [-0.00490494]\n",
      " [ 0.00929949]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.09880477]\n",
      " [-0.11367243]\n",
      " [ 0.84306891]]\n",
      "layer.weight.data: [[ 0.09880477]\n",
      " [-0.11367243]\n",
      " [ 0.84306891]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00340847]\n",
      " [-0.00319335]\n",
      " [-0.01275652]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.0953963 ]\n",
      " [-0.11047907]\n",
      " [ 0.85582543]]\n",
      "layer.weight.data: [[ 0.0953963 ]\n",
      " [-0.11047907]\n",
      " [ 0.85582543]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02535938]\n",
      " [-0.00475114]\n",
      " [ 0.00935396]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.12075569]\n",
      " [-0.10572794]\n",
      " [ 0.84647147]]\n",
      "layer.weight.data: [[ 0.12075569]\n",
      " [-0.10572794]\n",
      " [ 0.84647147]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00370995]\n",
      " [-0.00299181]\n",
      " [-0.01270132]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.11704574]\n",
      " [-0.10273613]\n",
      " [ 0.85917279]]\n",
      "layer.weight.data: [[ 0.11704574]\n",
      " [-0.10273613]\n",
      " [ 0.85917279]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02478383]\n",
      " [-0.00460404]\n",
      " [ 0.00940673]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.14182957]\n",
      " [-0.09813209]\n",
      " [ 0.84976607]]\n",
      "layer.weight.data: [[ 0.14182957]\n",
      " [-0.09813209]\n",
      " [ 0.84976607]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.0039993 ]\n",
      " [-0.00279893]\n",
      " [-0.01264772]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.13783027]\n",
      " [-0.09533316]\n",
      " [ 0.86241378]]\n",
      "layer.weight.data: [[ 0.13783027]\n",
      " [-0.09533316]\n",
      " [ 0.86241378]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02423128]\n",
      " [-0.00446338]\n",
      " [ 0.00945785]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.16206155]\n",
      " [-0.09086979]\n",
      " [ 0.85295594]]\n",
      "layer.weight.data: [[ 0.16206155]\n",
      " [-0.09086979]\n",
      " [ 0.85295594]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.004277  ]\n",
      " [-0.00261435]\n",
      " [-0.01259568]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.15778455]\n",
      " [-0.08825544]\n",
      " [ 0.86555162]]\n",
      "layer.weight.data: [[ 0.15778455]\n",
      " [-0.08825544]\n",
      " [ 0.86555162]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02370082]\n",
      " [-0.00432886]\n",
      " [ 0.00950737]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.18148537]\n",
      " [-0.08392658]\n",
      " [ 0.85604425]]\n",
      "layer.weight.data: [[ 0.18148537]\n",
      " [-0.08392658]\n",
      " [ 0.85604425]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00454354]\n",
      " [-0.00243771]\n",
      " [-0.01254518]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.17694184]\n",
      " [-0.08148887]\n",
      " [ 0.86858943]]\n",
      "layer.weight.data: [[ 0.17694184]\n",
      " [-0.08148887]\n",
      " [ 0.86858943]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02319157]\n",
      " [-0.00420022]\n",
      " [ 0.00955534]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.2001334 ]\n",
      " [-0.07728865]\n",
      " [ 0.85903409]]\n",
      "layer.weight.data: [[ 0.2001334 ]\n",
      " [-0.07728865]\n",
      " [ 0.85903409]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00479934]\n",
      " [-0.00226868]\n",
      " [-0.01249615]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.19533406]\n",
      " [-0.07501997]\n",
      " [ 0.87153024]]\n",
      "layer.weight.data: [[ 0.19533406]\n",
      " [-0.07501997]\n",
      " [ 0.87153024]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02270266]\n",
      " [-0.00407722]\n",
      " [ 0.0096018 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.21803672]\n",
      " [-0.07094275]\n",
      " [ 0.86192844]]\n",
      "layer.weight.data: [[ 0.21803672]\n",
      " [-0.07094275]\n",
      " [ 0.86192844]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00504486]\n",
      " [-0.00210693]\n",
      " [-0.01244857]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.21299186]\n",
      " [-0.06883582]\n",
      " [ 0.87437701]]\n",
      "layer.weight.data: [[ 0.21299186]\n",
      " [-0.06883582]\n",
      " [ 0.87437701]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02223329]\n",
      " [-0.0039596 ]\n",
      " [ 0.0096468 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.23522515]\n",
      " [-0.06487622]\n",
      " [ 0.8647302 ]]\n",
      "layer.weight.data: [[ 0.23522515]\n",
      " [-0.06487622]\n",
      " [ 0.8647302 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00528051]\n",
      " [-0.00195215]\n",
      " [-0.0124024 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.22994464]\n",
      " [-0.06292407]\n",
      " [ 0.87713261]]\n",
      "layer.weight.data: [[ 0.22994464]\n",
      " [-0.06292407]\n",
      " [ 0.87713261]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02178268]\n",
      " [-0.00384714]\n",
      " [ 0.00969039]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.25172732]\n",
      " [-0.05907693]\n",
      " [ 0.86744222]]\n",
      "layer.weight.data: [[ 0.25172732]\n",
      " [-0.05907693]\n",
      " [ 0.86744222]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00550667]\n",
      " [-0.00180404]\n",
      " [-0.0123576 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.24622064]\n",
      " [-0.05727289]\n",
      " [ 0.87979982]]\n",
      "layer.weight.data: [[ 0.24622064]\n",
      " [-0.05727289]\n",
      " [ 0.87979982]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02135006]\n",
      " [-0.00373961]\n",
      " [ 0.00973259]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.26757071]\n",
      " [-0.05353328]\n",
      " [ 0.87006723]]\n",
      "layer.weight.data: [[ 0.26757071]\n",
      " [-0.05353328]\n",
      " [ 0.87006723]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00572374]\n",
      " [-0.00166232]\n",
      " [-0.01231413]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.26184696]\n",
      " [-0.05187096]\n",
      " [ 0.88238136]]\n",
      "layer.weight.data: [[ 0.26184696]\n",
      " [-0.05187096]\n",
      " [ 0.88238136]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02093473]\n",
      " [-0.00363679]\n",
      " [ 0.00977347]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.28278169]\n",
      " [-0.04823417]\n",
      " [ 0.87260789]]\n",
      "layer.weight.data: [[ 0.28278169]\n",
      " [-0.04823417]\n",
      " [ 0.87260789]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00593209]\n",
      " [-0.00152672]\n",
      " [-0.01227196]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.27684961]\n",
      " [-0.04670746]\n",
      " [ 0.88487986]]\n",
      "layer.weight.data: [[ 0.27684961]\n",
      " [-0.04670746]\n",
      " [ 0.88487986]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02053598]\n",
      " [-0.00353849]\n",
      " [ 0.00981304]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.29738559]\n",
      " [-0.04316897]\n",
      " [ 0.87506682]]\n",
      "layer.weight.data: [[ 0.29738559]\n",
      " [-0.04316897]\n",
      " [ 0.87506682]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00613205]\n",
      " [-0.00139697]\n",
      " [-0.01223106]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.29125354]\n",
      " [-0.041772  ]\n",
      " [ 0.88729787]]\n",
      "layer.weight.data: [[ 0.29125354]\n",
      " [-0.041772  ]\n",
      " [ 0.88729787]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.02015316]\n",
      " [-0.00344451]\n",
      " [ 0.00985136]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.3114067 ]\n",
      " [-0.03832749]\n",
      " [ 0.87744651]]\n",
      "layer.weight.data: [[ 0.3114067 ]\n",
      " [-0.03832749]\n",
      " [ 0.87744651]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00632398]\n",
      " [-0.00127282]\n",
      " [-0.01219138]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.30508272]\n",
      " [-0.03705468]\n",
      " [ 0.88963789]]\n",
      "layer.weight.data: [[ 0.30508272]\n",
      " [-0.03705468]\n",
      " [ 0.88963789]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01978563]\n",
      " [-0.00335465]\n",
      " [ 0.00988846]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.32486835]\n",
      " [-0.03370002]\n",
      " [ 0.87974943]]\n",
      "layer.weight.data: [[ 0.32486835]\n",
      " [-0.03370002]\n",
      " [ 0.87974943]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00650819]\n",
      " [-0.00115403]\n",
      " [-0.01215289]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.31836016]\n",
      " [-0.03254599]\n",
      " [ 0.89190232]]\n",
      "layer.weight.data: [[ 0.31836016]\n",
      " [-0.03254599]\n",
      " [ 0.89190232]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01943277]\n",
      " [-0.00326876]\n",
      " [ 0.00992438]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.33779293]\n",
      " [-0.02927723]\n",
      " [ 0.88197794]]\n",
      "layer.weight.data: [[ 0.33779293]\n",
      " [-0.02927723]\n",
      " [ 0.88197794]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00668499]\n",
      " [-0.00104038]\n",
      " [-0.01211557]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.33110794]\n",
      " [-0.02823685]\n",
      " [ 0.89409351]]\n",
      "layer.weight.data: [[ 0.33110794]\n",
      " [-0.02823685]\n",
      " [ 0.89409351]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01909399]\n",
      " [-0.00318664]\n",
      " [ 0.00995915]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.35020193]\n",
      " [-0.02505021]\n",
      " [ 0.88413436]]\n",
      "layer.weight.data: [[ 0.35020193]\n",
      " [-0.02505021]\n",
      " [ 0.88413436]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00685469]\n",
      " [-0.00093165]\n",
      " [-0.01207938]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.34334723]\n",
      " [-0.02411856]\n",
      " [ 0.89621374]]\n",
      "layer.weight.data: [[ 0.34334723]\n",
      " [-0.02411856]\n",
      " [ 0.89621374]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01876874]\n",
      " [-0.00310813]\n",
      " [ 0.00999281]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.36211597]\n",
      " [-0.02101043]\n",
      " [ 0.88622092]]\n",
      "layer.weight.data: [[ 0.36211597]\n",
      " [-0.02101043]\n",
      " [ 0.88622092]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00701757]\n",
      " [-0.00082762]\n",
      " [-0.01204428]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.3550984 ]\n",
      " [-0.02018281]\n",
      " [ 0.8982652 ]]\n",
      "layer.weight.data: [[ 0.3550984 ]\n",
      " [-0.02018281]\n",
      " [ 0.8982652 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01845647]\n",
      " [-0.00303309]\n",
      " [ 0.0100254 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.37355486]\n",
      " [-0.01714971]\n",
      " [ 0.8882398 ]]\n",
      "layer.weight.data: [[ 0.37355486]\n",
      " [-0.01714971]\n",
      " [ 0.8882398 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00717391]\n",
      " [-0.00072809]\n",
      " [-0.01201025]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.36638096]\n",
      " [-0.01642162]\n",
      " [ 0.90025005]]\n",
      "layer.weight.data: [[ 0.36638096]\n",
      " [-0.01642162]\n",
      " [ 0.90025005]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01815665]\n",
      " [-0.00296136]\n",
      " [ 0.01005694]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.38453761]\n",
      " [-0.01346026]\n",
      " [ 0.89019312]]\n",
      "layer.weight.data: [[ 0.38453761]\n",
      " [-0.01346026]\n",
      " [ 0.89019312]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00732396]\n",
      " [-0.00063287]\n",
      " [-0.01197726]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.37721365]\n",
      " [-0.01282739]\n",
      " [ 0.90217038]]\n",
      "layer.weight.data: [[ 0.37721365]\n",
      " [-0.01282739]\n",
      " [ 0.90217038]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01786881]\n",
      " [-0.00289279]\n",
      " [ 0.01008747]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.39508246]\n",
      " [-0.00993459]\n",
      " [ 0.89208291]]\n",
      "layer.weight.data: [[ 0.39508246]\n",
      " [-0.00993459]\n",
      " [ 0.89208291]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00746799]\n",
      " [-0.00054178]\n",
      " [-0.01194528]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.38761447]\n",
      " [-0.00939281]\n",
      " [ 0.90402819]]\n",
      "layer.weight.data: [[ 0.38761447]\n",
      " [-0.00939281]\n",
      " [ 0.90402819]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01759244]\n",
      " [-0.00282726]\n",
      " [ 0.01011701]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.40520691]\n",
      " [-0.00656556]\n",
      " [ 0.89391118]]\n",
      "layer.weight.data: [[ 0.40520691]\n",
      " [-0.00656556]\n",
      " [ 0.89391118]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00760622]\n",
      " [-0.00045464]\n",
      " [-0.01191428]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.39760069]\n",
      " [-0.00611092]\n",
      " [ 0.90582545]]\n",
      "layer.weight.data: [[ 0.39760069]\n",
      " [-0.00611092]\n",
      " [ 0.90582545]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.0173271 ]\n",
      " [-0.00276461]\n",
      " [ 0.01014561]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.41492779]\n",
      " [-0.0033463 ]\n",
      " [ 0.89567985]]\n",
      "layer.weight.data: [[ 0.41492779]\n",
      " [-0.0033463 ]\n",
      " [ 0.89567985]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00773891]\n",
      " [-0.00037127]\n",
      " [-0.01188423]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 0.40718888]\n",
      " [-0.00297503]\n",
      " [ 0.90756408]]\n",
      "layer.weight.data: [[ 0.40718888]\n",
      " [-0.00297503]\n",
      " [ 0.90756408]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01707234]\n",
      " [-0.00270474]\n",
      " [ 0.01017328]], requires_grad=False)\n",
      "after update layer.weight.data: [[ 4.24261225e-01]\n",
      " [-2.70285168e-04]\n",
      " [ 8.97390798e-01]]\n",
      "layer.weight.data: [[ 4.24261225e-01]\n",
      " [-2.70285168e-04]\n",
      " [ 8.97390798e-01]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00786627]\n",
      " [-0.00029153]\n",
      " [-0.0118551 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[4.16394957e-01]\n",
      " [2.12401976e-05]\n",
      " [9.09245898e-01]]\n",
      "layer.weight.data: [[4.16394957e-01]\n",
      " [2.12401976e-05]\n",
      " [9.09245898e-01]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01682775]\n",
      " [-0.00264752]\n",
      " [ 0.01020006]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.4332227 ]\n",
      " [0.00266876]\n",
      " [0.89904584]]\n",
      "layer.weight.data: [[0.4332227 ]\n",
      " [0.00266876]\n",
      " [0.89904584]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00798851]\n",
      " [-0.00021524]\n",
      " [-0.01182687]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.42523419]\n",
      " [0.002884  ]\n",
      " [0.91087271]]\n",
      "layer.weight.data: [[0.42523419]\n",
      " [0.002884  ]\n",
      " [0.91087271]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.0165929 ]\n",
      " [-0.00259284]\n",
      " [ 0.01022597]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.4418271 ]\n",
      " [0.00547684]\n",
      " [0.90064675]]\n",
      "layer.weight.data: [[0.4418271 ]\n",
      " [0.00547684]\n",
      " [0.90064675]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00810584]\n",
      " [-0.00014227]\n",
      " [-0.01179952]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.43372125]\n",
      " [0.0056191 ]\n",
      " [0.91244626]]\n",
      "layer.weight.data: [[0.43372125]\n",
      " [0.0056191 ]\n",
      " [0.91244626]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01636742]\n",
      " [-0.00254057]\n",
      " [ 0.01025104]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.45008868]\n",
      " [0.00815968]\n",
      " [0.90219522]]\n",
      "layer.weight.data: [[0.45008868]\n",
      " [0.00815968]\n",
      " [0.90219522]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 8.21846739e-03]\n",
      " [-7.24644641e-05]\n",
      " [-1.17730090e-02]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.44187021]\n",
      " [0.00823214]\n",
      " [0.91396823]]\n",
      "layer.weight.data: [[0.44187021]\n",
      " [0.00823214]\n",
      " [0.91396823]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01615093]\n",
      " [-0.00249063]\n",
      " [ 0.0102753 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.45802114]\n",
      " [0.01072277]\n",
      " [0.90369293]]\n",
      "layer.weight.data: [[0.45802114]\n",
      " [0.01072277]\n",
      " [0.90369293]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 8.32656857e-03]\n",
      " [-5.69700409e-06]\n",
      " [-1.17473238e-02]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.44969457]\n",
      " [0.01072847]\n",
      " [0.91544025]]\n",
      "layer.weight.data: [[0.44969457]\n",
      " [0.01072847]\n",
      " [0.91544025]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01594307]\n",
      " [-0.0024429 ]\n",
      " [ 0.01029877]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.46563764]\n",
      " [0.01317137]\n",
      " [0.90514148]]\n",
      "layer.weight.data: [[0.46563764]\n",
      " [0.01317137]\n",
      " [0.90514148]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 8.43033021e-03]\n",
      " [ 5.81663191e-05]\n",
      " [-1.17224375e-02]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.45720731]\n",
      " [0.0131132 ]\n",
      " [0.91686392]]\n",
      "layer.weight.data: [[0.45720731]\n",
      " [0.0131132 ]\n",
      " [0.91686392]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01574349]\n",
      " [-0.00239729]\n",
      " [ 0.01032148]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.4729508 ]\n",
      " [0.0155105 ]\n",
      " [0.90654243]]\n",
      "layer.weight.data: [[0.4729508 ]\n",
      " [0.0155105 ]\n",
      " [0.90654243]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00852993]\n",
      " [ 0.00011925]\n",
      " [-0.01169833]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.46442087]\n",
      " [0.01539124]\n",
      " [0.91824076]]\n",
      "layer.weight.data: [[0.46442087]\n",
      " [0.01539124]\n",
      " [0.91824076]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01555187]\n",
      " [-0.00235371]\n",
      " [ 0.01034345]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.47997274]\n",
      " [0.01774496]\n",
      " [0.90789731]]\n",
      "layer.weight.data: [[0.47997274]\n",
      " [0.01774496]\n",
      " [0.90789731]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00862553]\n",
      " [ 0.00017767]\n",
      " [-0.01167497]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.47134721]\n",
      " [0.01756728]\n",
      " [0.91957228]]\n",
      "layer.weight.data: [[0.47134721]\n",
      " [0.01756728]\n",
      " [0.91957228]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01536788]\n",
      " [-0.00231207]\n",
      " [ 0.01036471]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.48671509]\n",
      " [0.01987936]\n",
      " [0.90920757]]\n",
      "layer.weight.data: [[0.48671509]\n",
      " [0.01987936]\n",
      " [0.90920757]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00871729]\n",
      " [ 0.00023355]\n",
      " [-0.01165234]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.4779978 ]\n",
      " [0.0196458 ]\n",
      " [0.92085991]]\n",
      "layer.weight.data: [[0.4779978 ]\n",
      " [0.0196458 ]\n",
      " [0.92085991]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01519122]\n",
      " [-0.00227229]\n",
      " [ 0.01038527]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.49318902]\n",
      " [0.02191809]\n",
      " [0.91047464]]\n",
      "layer.weight.data: [[0.49318902]\n",
      " [0.02191809]\n",
      " [0.91047464]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00880537]\n",
      " [ 0.000287  ]\n",
      " [-0.01163042]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.48438365]\n",
      " [0.02163109]\n",
      " [0.92210506]]\n",
      "layer.weight.data: [[0.48438365]\n",
      " [0.02163109]\n",
      " [0.92210506]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01502159]\n",
      " [-0.00223427]\n",
      " [ 0.01040516]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.49940524]\n",
      " [0.02386537]\n",
      " [0.91169991]]\n",
      "layer.weight.data: [[0.49940524]\n",
      " [0.02386537]\n",
      " [0.91169991]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00888992]\n",
      " [ 0.00033811]\n",
      " [-0.01160919]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.49051532]\n",
      " [0.02352726]\n",
      " [0.9233091 ]]\n",
      "layer.weight.data: [[0.49051532]\n",
      " [0.02352726]\n",
      " [0.9233091 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01485873]\n",
      " [-0.00219796]\n",
      " [ 0.0104244 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.50537405]\n",
      " [0.02572521]\n",
      " [0.9128847 ]]\n",
      "layer.weight.data: [[0.50537405]\n",
      " [0.02572521]\n",
      " [0.9128847 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00897107]\n",
      " [ 0.00038699]\n",
      " [-0.01158863]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.49640298]\n",
      " [0.02533822]\n",
      " [0.92447334]]\n",
      "layer.weight.data: [[0.49640298]\n",
      " [0.02533822]\n",
      " [0.92447334]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01470235]\n",
      " [-0.00216326]\n",
      " [ 0.010443  ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.51110533]\n",
      " [0.02750148]\n",
      " [0.91403033]]\n",
      "layer.weight.data: [[0.51110533]\n",
      " [0.02750148]\n",
      " [0.91403033]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00904897]\n",
      " [ 0.00043374]\n",
      " [-0.01156872]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.50205635]\n",
      " [0.02706773]\n",
      " [0.92559905]]\n",
      "layer.weight.data: [[0.50205635]\n",
      " [0.02706773]\n",
      " [0.92559905]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01455219]\n",
      " [-0.00213011]\n",
      " [ 0.010461  ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.51660855]\n",
      " [0.02919785]\n",
      " [0.91513805]]\n",
      "layer.weight.data: [[0.51660855]\n",
      " [0.02919785]\n",
      " [0.91513805]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00912375]\n",
      " [ 0.00047845]\n",
      " [-0.01154943]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.5074848 ]\n",
      " [0.0287194 ]\n",
      " [0.92668748]]\n",
      "layer.weight.data: [[0.5074848 ]\n",
      " [0.0287194 ]\n",
      " [0.92668748]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01440802]\n",
      " [-0.00209845]\n",
      " [ 0.01047841]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.52189282]\n",
      " [0.03081785]\n",
      " [0.91620907]]\n",
      "layer.weight.data: [[0.52189282]\n",
      " [0.03081785]\n",
      " [0.91620907]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00919552]\n",
      " [ 0.0005212 ]\n",
      " [-0.01153076]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.5126973 ]\n",
      " [0.03029665]\n",
      " [0.92773983]]\n",
      "layer.weight.data: [[0.5126973 ]\n",
      " [0.03029665]\n",
      " [0.92773983]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01426958]\n",
      " [-0.0020682 ]\n",
      " [ 0.01049524]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.52696688]\n",
      " [0.03236484]\n",
      " [0.91724459]]\n",
      "layer.weight.data: [[0.52696688]\n",
      " [0.03236484]\n",
      " [0.91724459]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00926442]\n",
      " [ 0.00056208]\n",
      " [-0.01151267]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.51770246]\n",
      " [0.03180276]\n",
      " [0.92875726]]\n",
      "layer.weight.data: [[0.51770246]\n",
      " [0.03180276]\n",
      " [0.92875726]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01413666]\n",
      " [-0.00203931]\n",
      " [ 0.01051152]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.53183912]\n",
      " [0.03384207]\n",
      " [0.91824573]]\n",
      "layer.weight.data: [[0.53183912]\n",
      " [0.03384207]\n",
      " [0.91824573]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00933055]\n",
      " [ 0.00060118]\n",
      " [-0.01149516]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.52250856]\n",
      " [0.03324089]\n",
      " [0.9297409 ]]\n",
      "layer.weight.data: [[0.52250856]\n",
      " [0.03324089]\n",
      " [0.9297409 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01400902]\n",
      " [-0.00201171]\n",
      " [ 0.01052727]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.53651759]\n",
      " [0.0352526 ]\n",
      " [0.91921363]]\n",
      "layer.weight.data: [[0.53651759]\n",
      " [0.0352526 ]\n",
      " [0.91921363]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00939404]\n",
      " [ 0.00063856]\n",
      " [-0.01147821]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.52712355]\n",
      " [0.03461404]\n",
      " [0.93069183]]\n",
      "layer.weight.data: [[0.52712355]\n",
      " [0.03461404]\n",
      " [0.93069183]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01388647]\n",
      " [-0.00198535]\n",
      " [ 0.0105425 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.54101001]\n",
      " [0.03659939]\n",
      " [0.92014934]]\n",
      "layer.weight.data: [[0.54101001]\n",
      " [0.03659939]\n",
      " [0.92014934]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00945498]\n",
      " [ 0.0006743 ]\n",
      " [-0.01146179]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.53155504]\n",
      " [0.03592509]\n",
      " [0.93161113]]\n",
      "layer.weight.data: [[0.53155504]\n",
      " [0.03592509]\n",
      " [0.93161113]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01376879]\n",
      " [-0.00196017]\n",
      " [ 0.01055722]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.54532382]\n",
      " [0.03788526]\n",
      " [0.92105391]]\n",
      "layer.weight.data: [[0.54532382]\n",
      " [0.03788526]\n",
      " [0.92105391]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00951347]\n",
      " [ 0.00070848]\n",
      " [-0.0114459 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.53581035]\n",
      " [0.03717678]\n",
      " [0.93249982]]\n",
      "layer.weight.data: [[0.53581035]\n",
      " [0.03717678]\n",
      " [0.93249982]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01365579]\n",
      " [-0.00193613]\n",
      " [ 0.01057146]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.54946614]\n",
      " [0.03911291]\n",
      " [0.92192836]]\n",
      "layer.weight.data: [[0.54946614]\n",
      " [0.03911291]\n",
      " [0.92192836]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00956962]\n",
      " [ 0.00074116]\n",
      " [-0.01143052]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.53989652]\n",
      " [0.03837175]\n",
      " [0.93335888]]\n",
      "layer.weight.data: [[0.53989652]\n",
      " [0.03837175]\n",
      " [0.93335888]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01354728]\n",
      " [-0.00191317]\n",
      " [ 0.01058522]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.5534438 ]\n",
      " [0.04028491]\n",
      " [0.92277366]]\n",
      "layer.weight.data: [[0.5534438 ]\n",
      " [0.04028491]\n",
      " [0.92277366]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00962352]\n",
      " [ 0.0007724 ]\n",
      " [-0.01141563]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.54382028]\n",
      " [0.03951251]\n",
      " [0.93418928]]\n",
      "layer.weight.data: [[0.54382028]\n",
      " [0.03951251]\n",
      " [0.93418928]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.0134431 ]\n",
      " [-0.00189124]\n",
      " [ 0.01059853]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.55726337]\n",
      " [0.04140374]\n",
      " [0.92359075]]\n",
      "layer.weight.data: [[0.55726337]\n",
      " [0.04140374]\n",
      " [0.92359075]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00967526]\n",
      " [ 0.00080228]\n",
      " [-0.01140121]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.54758811]\n",
      " [0.04060147]\n",
      " [0.93499196]]\n",
      "layer.weight.data: [[0.54758811]\n",
      " [0.04060147]\n",
      " [0.93499196]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01334305]\n",
      " [-0.0018703 ]\n",
      " [ 0.0106114 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.56093116]\n",
      " [0.04247176]\n",
      " [0.92438056]]\n",
      "layer.weight.data: [[0.56093116]\n",
      " [0.04247176]\n",
      " [0.92438056]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00972493]\n",
      " [ 0.00083084]\n",
      " [-0.01138726]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.55120622]\n",
      " [0.04164093]\n",
      " [0.93576782]]\n",
      "layer.weight.data: [[0.55120622]\n",
      " [0.04164093]\n",
      " [0.93576782]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01324698]\n",
      " [-0.0018503 ]\n",
      " [ 0.01062384]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.56445321]\n",
      " [0.04349123]\n",
      " [0.92514398]]\n",
      "layer.weight.data: [[0.56445321]\n",
      " [0.04349123]\n",
      " [0.92514398]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00977261]\n",
      " [ 0.00085814]\n",
      " [-0.01137376]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.5546806 ]\n",
      " [0.04263309]\n",
      " [0.93651773]]\n",
      "layer.weight.data: [[0.5546806 ]\n",
      " [0.04263309]\n",
      " [0.93651773]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01315474]\n",
      " [-0.00183121]\n",
      " [ 0.01063587]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.56783533]\n",
      " [0.04446429]\n",
      " [0.92588186]]\n",
      "layer.weight.data: [[0.56783533]\n",
      " [0.04446429]\n",
      " [0.92588186]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00981838]\n",
      " [ 0.00088424]\n",
      " [-0.01136068]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.55801695]\n",
      " [0.04358005]\n",
      " [0.93724254]]\n",
      "layer.weight.data: [[0.55801695]\n",
      " [0.04358005]\n",
      " [0.93724254]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01306616]\n",
      " [-0.00181298]\n",
      " [ 0.0106475 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.57108311]\n",
      " [0.04539303]\n",
      " [0.92659504]]\n",
      "layer.weight.data: [[0.57108311]\n",
      " [0.04539303]\n",
      " [0.92659504]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00986231]\n",
      " [ 0.0009092 ]\n",
      " [-0.01134804]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.5612208 ]\n",
      " [0.04448383]\n",
      " [0.93794308]]\n",
      "layer.weight.data: [[0.5612208 ]\n",
      " [0.04448383]\n",
      " [0.93794308]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.0129811 ]\n",
      " [-0.00179557]\n",
      " [ 0.01065874]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.57420189]\n",
      " [0.0462794 ]\n",
      " [0.92728433]]\n",
      "layer.weight.data: [[0.57420189]\n",
      " [0.0462794 ]\n",
      " [0.92728433]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00990449]\n",
      " [ 0.00093305]\n",
      " [-0.01133579]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.5642974 ]\n",
      " [0.04534635]\n",
      " [0.93862013]]\n",
      "layer.weight.data: [[0.5642974 ]\n",
      " [0.04534635]\n",
      " [0.93862013]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01289942]\n",
      " [-0.00177896]\n",
      " [ 0.01066961]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.57719682]\n",
      " [0.04712531]\n",
      " [0.92795051]]\n",
      "layer.weight.data: [[0.57719682]\n",
      " [0.04712531]\n",
      " [0.92795051]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00994498]\n",
      " [ 0.00095586]\n",
      " [-0.01132395]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.56725184]\n",
      " [0.04616945]\n",
      " [0.93927446]]\n",
      "layer.weight.data: [[0.56725184]\n",
      " [0.04616945]\n",
      " [0.93927446]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01282098]\n",
      " [-0.00176309]\n",
      " [ 0.01068012]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.58007283]\n",
      " [0.04793254]\n",
      " [0.92859434]]\n",
      "layer.weight.data: [[0.58007283]\n",
      " [0.04793254]\n",
      " [0.92859434]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.00998384]\n",
      " [ 0.00097765]\n",
      " [-0.01131248]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.57008898]\n",
      " [0.04695489]\n",
      " [0.93990683]]\n",
      "layer.weight.data: [[0.57008898]\n",
      " [0.04695489]\n",
      " [0.93990683]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01274566]\n",
      " [-0.00174795]\n",
      " [ 0.01069027]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.58283465]\n",
      " [0.04870284]\n",
      " [0.92921655]]\n",
      "layer.weight.data: [[0.58283465]\n",
      " [0.04870284]\n",
      " [0.92921655]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01002115]\n",
      " [ 0.00099849]\n",
      " [-0.01130139]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.5728135 ]\n",
      " [0.04770435]\n",
      " [0.94051794]]\n",
      "layer.weight.data: [[0.5728135 ]\n",
      " [0.04770435]\n",
      " [0.94051794]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01267334]\n",
      " [-0.0017335 ]\n",
      " [ 0.01070009]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.58548684]\n",
      " [0.04943785]\n",
      " [0.92981785]]\n",
      "layer.weight.data: [[0.58548684]\n",
      " [0.04943785]\n",
      " [0.92981785]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01005697]\n",
      " [ 0.0010184 ]\n",
      " [-0.01129066]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.57542987]\n",
      " [0.04841945]\n",
      " [0.94110851]]\n",
      "layer.weight.data: [[0.57542987]\n",
      " [0.04841945]\n",
      " [0.94110851]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01260388]\n",
      " [-0.0017197 ]\n",
      " [ 0.01070958]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.58803376]\n",
      " [0.05013914]\n",
      " [0.93039893]]\n",
      "layer.weight.data: [[0.58803376]\n",
      " [0.05013914]\n",
      " [0.93039893]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01009135]\n",
      " [ 0.00103744]\n",
      " [-0.01128028]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.57794241]\n",
      " [0.0491017 ]\n",
      " [0.94167921]]\n",
      "layer.weight.data: [[0.57794241]\n",
      " [0.0491017 ]\n",
      " [0.94167921]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01253719]\n",
      " [-0.00170653]\n",
      " [ 0.01071875]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.5904796 ]\n",
      " [0.05080823]\n",
      " [0.93096046]]\n",
      "layer.weight.data: [[0.5904796 ]\n",
      " [0.05080823]\n",
      " [0.93096046]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01012435]\n",
      " [ 0.00105563]\n",
      " [-0.01127023]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.58035525]\n",
      " [0.0497526 ]\n",
      " [0.94223069]]\n",
      "layer.weight.data: [[0.58035525]\n",
      " [0.0497526 ]\n",
      " [0.94223069]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01247314]\n",
      " [-0.00169396]\n",
      " [ 0.01072761]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.59282839]\n",
      " [0.05144656]\n",
      " [0.93150307]]\n",
      "layer.weight.data: [[0.59282839]\n",
      " [0.05144656]\n",
      " [0.93150307]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01015604]\n",
      " [ 0.00107302]\n",
      " [-0.01126051]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.58267235]\n",
      " [0.05037354]\n",
      " [0.94276358]]\n",
      "layer.weight.data: [[0.58267235]\n",
      " [0.05037354]\n",
      " [0.94276358]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01241164]\n",
      " [-0.00168196]\n",
      " [ 0.01073618]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.59508399]\n",
      " [0.05205551]\n",
      " [0.9320274 ]]\n",
      "layer.weight.data: [[0.59508399]\n",
      " [0.05205551]\n",
      " [0.9320274 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01018645]\n",
      " [ 0.00108964]\n",
      " [-0.01125111]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.58489754]\n",
      " [0.05096587]\n",
      " [0.94327851]]\n",
      "layer.weight.data: [[0.58489754]\n",
      " [0.05096587]\n",
      " [0.94327851]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01235258]\n",
      " [-0.00167052]\n",
      " [ 0.01074446]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.59725012]\n",
      " [0.05263638]\n",
      " [0.93253404]]\n",
      "layer.weight.data: [[0.59725012]\n",
      " [0.05263638]\n",
      " [0.93253404]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01021565]\n",
      " [ 0.00110552]\n",
      " [-0.01124201]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.58703446]\n",
      " [0.05153087]\n",
      " [0.94377605]]\n",
      "layer.weight.data: [[0.58703446]\n",
      " [0.05153087]\n",
      " [0.94377605]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01229586]\n",
      " [-0.00165959]\n",
      " [ 0.01075246]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.59933032]\n",
      " [0.05319046]\n",
      " [0.93302359]]\n",
      "layer.weight.data: [[0.59933032]\n",
      " [0.05319046]\n",
      " [0.93302359]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01024369]\n",
      " [ 0.0011207 ]\n",
      " [-0.01123321]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.58908663]\n",
      " [0.05206976]\n",
      " [0.9442568 ]]\n",
      "layer.weight.data: [[0.58908663]\n",
      " [0.05206976]\n",
      " [0.9442568 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01224139]\n",
      " [-0.00164917]\n",
      " [ 0.0107602 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60132803]\n",
      " [0.05371893]\n",
      " [0.9334966 ]]\n",
      "layer.weight.data: [[0.60132803]\n",
      " [0.05371893]\n",
      " [0.9334966 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.0102706]\n",
      " [ 0.0011352]\n",
      " [-0.0112247]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.59105743]\n",
      " [0.05258373]\n",
      " [0.9447213 ]]\n",
      "layer.weight.data: [[0.59105743]\n",
      " [0.05258373]\n",
      " [0.9447213 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01218909]\n",
      " [-0.00163922]\n",
      " [ 0.01076767]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60324652]\n",
      " [0.05422295]\n",
      " [0.93395363]]\n",
      "layer.weight.data: [[0.60324652]\n",
      " [0.05422295]\n",
      " [0.93395363]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01029643]\n",
      " [ 0.00114906]\n",
      " [-0.01121646]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.59295009]\n",
      " [0.0530739 ]\n",
      " [0.94517009]]\n",
      "layer.weight.data: [[0.59295009]\n",
      " [0.0530739 ]\n",
      " [0.94517009]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01213886]\n",
      " [-0.00162973]\n",
      " [ 0.01077489]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60508894]\n",
      " [0.05470363]\n",
      " [0.9343952 ]]\n",
      "layer.weight.data: [[0.60508894]\n",
      " [0.05470363]\n",
      " [0.9343952 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01032123]\n",
      " [ 0.0011623 ]\n",
      " [-0.0112085 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.59476771]\n",
      " [0.05354133]\n",
      " [0.9456037 ]]\n",
      "layer.weight.data: [[0.59476771]\n",
      " [0.05354133]\n",
      " [0.9456037 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01209062]\n",
      " [-0.00162068]\n",
      " [ 0.01078187]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60685833]\n",
      " [0.055162  ]\n",
      " [0.93482183]]\n",
      "layer.weight.data: [[0.60685833]\n",
      " [0.055162  ]\n",
      " [0.93482183]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01034504]\n",
      " [ 0.00117495]\n",
      " [-0.0112008 ]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.59651329]\n",
      " [0.05398705]\n",
      " [0.94602263]]\n",
      "layer.weight.data: [[0.59651329]\n",
      " [0.05398705]\n",
      " [0.94602263]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.0120443 ]\n",
      " [-0.00161204]\n",
      " [ 0.01078862]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60855758]\n",
      " [0.05559909]\n",
      " [0.93523401]]\n",
      "layer.weight.data: [[0.60855758]\n",
      " [0.05559909]\n",
      " [0.93523401]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.0103679 ]\n",
      " [ 0.00118704]\n",
      " [-0.01119335]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.59818968]\n",
      " [0.05441205]\n",
      " [0.94642736]]\n",
      "layer.weight.data: [[0.59818968]\n",
      " [0.05441205]\n",
      " [0.94642736]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01199981]\n",
      " [-0.0016038 ]\n",
      " [ 0.01079513]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61018949]\n",
      " [0.05601585]\n",
      " [0.93563223]]\n",
      "layer.weight.data: [[0.61018949]\n",
      " [0.05601585]\n",
      " [0.93563223]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01038984]\n",
      " [ 0.0011986 ]\n",
      " [-0.01118614]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.59979965]\n",
      " [0.05481725]\n",
      " [0.94681837]]\n",
      "layer.weight.data: [[0.59979965]\n",
      " [0.05481725]\n",
      " [0.94681837]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01195709]\n",
      " [-0.00159594]\n",
      " [ 0.01080143]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61175673]\n",
      " [0.05641319]\n",
      " [0.93601694]]\n",
      "layer.weight.data: [[0.61175673]\n",
      " [0.05641319]\n",
      " [0.93601694]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01041091]\n",
      " [ 0.00120963]\n",
      " [-0.01117918]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60134582]\n",
      " [0.05520356]\n",
      " [0.94719612]]\n",
      "layer.weight.data: [[0.60134582]\n",
      " [0.05520356]\n",
      " [0.94719612]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01191606]\n",
      " [-0.00158845]\n",
      " [ 0.01080751]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61326188]\n",
      " [0.056792  ]\n",
      " [0.9363886 ]]\n",
      "layer.weight.data: [[0.61326188]\n",
      " [0.056792  ]\n",
      " [0.9363886 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01043113]\n",
      " [ 0.00122018]\n",
      " [-0.01117244]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60283075]\n",
      " [0.05557183]\n",
      " [0.94756104]]\n",
      "layer.weight.data: [[0.60283075]\n",
      " [0.05557183]\n",
      " [0.94756104]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01187665]\n",
      " [-0.0015813 ]\n",
      " [ 0.01081339]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.6147074 ]\n",
      " [0.05715312]\n",
      " [0.93674765]]\n",
      "layer.weight.data: [[0.6147074 ]\n",
      " [0.05715312]\n",
      " [0.93674765]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01045055]\n",
      " [ 0.00123025]\n",
      " [-0.01116592]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60425685]\n",
      " [0.05592287]\n",
      " [0.94791357]]\n",
      "layer.weight.data: [[0.60425685]\n",
      " [0.05592287]\n",
      " [0.94791357]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01183881]\n",
      " [-0.00157448]\n",
      " [ 0.01081907]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61609566]\n",
      " [0.05749735]\n",
      " [0.9370945 ]]\n",
      "layer.weight.data: [[0.61609566]\n",
      " [0.05749735]\n",
      " [0.9370945 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01046919]\n",
      " [ 0.00123987]\n",
      " [-0.01115962]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60562647]\n",
      " [0.05625747]\n",
      " [0.94825412]]\n",
      "layer.weight.data: [[0.60562647]\n",
      " [0.05625747]\n",
      " [0.94825412]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01180247]\n",
      " [-0.00156797]\n",
      " [ 0.01082456]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61742895]\n",
      " [0.05782545]\n",
      " [0.93742956]]\n",
      "layer.weight.data: [[0.61742895]\n",
      " [0.05782545]\n",
      " [0.93742956]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01048708]\n",
      " [ 0.00124907]\n",
      " [-0.01115353]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60694186]\n",
      " [0.05657638]\n",
      " [0.94858308]]\n",
      "layer.weight.data: [[0.60694186]\n",
      " [0.05657638]\n",
      " [0.94858308]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01176757]\n",
      " [-0.00156177]\n",
      " [ 0.01082986]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61870943]\n",
      " [0.05813815]\n",
      " [0.93775322]]\n",
      "layer.weight.data: [[0.61870943]\n",
      " [0.05813815]\n",
      " [0.93775322]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01050427]\n",
      " [ 0.00125785]\n",
      " [-0.01114764]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60820517]\n",
      " [0.05688031]\n",
      " [0.94890086]]\n",
      "layer.weight.data: [[0.60820517]\n",
      " [0.05688031]\n",
      " [0.94890086]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01173405]\n",
      " [-0.00155586]\n",
      " [ 0.01083498]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61993922]\n",
      " [0.05843617]\n",
      " [0.93806587]]\n",
      "layer.weight.data: [[0.61993922]\n",
      " [0.05843617]\n",
      " [0.93806587]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01052076]\n",
      " [ 0.00126624]\n",
      " [-0.01114194]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.60941846]\n",
      " [0.05716993]\n",
      " [0.94920781]]\n",
      "layer.weight.data: [[0.60941846]\n",
      " [0.05716993]\n",
      " [0.94920781]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01170186]\n",
      " [-0.00155022]\n",
      " [ 0.01083993]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.62112032]\n",
      " [0.05872015]\n",
      " [0.93836788]]\n",
      "layer.weight.data: [[0.62112032]\n",
      " [0.05872015]\n",
      " [0.93836788]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01053659]\n",
      " [ 0.00127425]\n",
      " [-0.01113643]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61058373]\n",
      " [0.05744591]\n",
      " [0.94950431]]\n",
      "layer.weight.data: [[0.61058373]\n",
      " [0.05744591]\n",
      " [0.94950431]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01167095]\n",
      " [-0.00154485]\n",
      " [ 0.01084471]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.62225467]\n",
      " [0.05899075]\n",
      " [0.9386596 ]]\n",
      "layer.weight.data: [[0.62225467]\n",
      " [0.05899075]\n",
      " [0.9386596 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.0105518 ]\n",
      " [ 0.0012819 ]\n",
      " [-0.01113111]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61170288]\n",
      " [0.05770885]\n",
      " [0.9497907 ]]\n",
      "layer.weight.data: [[0.61170288]\n",
      " [0.05770885]\n",
      " [0.9497907 ]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01164126]\n",
      " [-0.00153972]\n",
      " [ 0.01084933]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.62334413]\n",
      " [0.05924858]\n",
      " [0.93894137]]\n",
      "layer.weight.data: [[0.62334413]\n",
      " [0.05924858]\n",
      " [0.93894137]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01056639]\n",
      " [ 0.00128921]\n",
      " [-0.01112596]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61277774]\n",
      " [0.05795937]\n",
      " [0.95006733]]\n",
      "layer.weight.data: [[0.61277774]\n",
      " [0.05795937]\n",
      " [0.95006733]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01161274]\n",
      " [-0.00153484]\n",
      " [ 0.01085379]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.62439048]\n",
      " [0.05949421]\n",
      " [0.93921353]]\n",
      "layer.weight.data: [[0.62439048]\n",
      " [0.05949421]\n",
      " [0.93921353]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01058041]\n",
      " [ 0.00129619]\n",
      " [-0.01112098]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61381007]\n",
      " [0.05819802]\n",
      " [0.95033451]]\n",
      "layer.weight.data: [[0.61381007]\n",
      " [0.05819802]\n",
      " [0.95033451]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01158535]\n",
      " [-0.00153019]\n",
      " [ 0.01085811]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.62539543]\n",
      " [0.05972821]\n",
      " [0.93947641]]\n",
      "layer.weight.data: [[0.62539543]\n",
      " [0.05972821]\n",
      " [0.93947641]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01059386]\n",
      " [ 0.00130285]\n",
      " [-0.01111617]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61480157]\n",
      " [0.05842536]\n",
      " [0.95059257]]\n",
      "layer.weight.data: [[0.61480157]\n",
      " [0.05842536]\n",
      " [0.95059257]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01155905]\n",
      " [-0.00152575]\n",
      " [ 0.01086227]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.62636062]\n",
      " [0.0599511 ]\n",
      " [0.93973031]]\n",
      "layer.weight.data: [[0.62636062]\n",
      " [0.0599511 ]\n",
      " [0.93973031]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01060678]\n",
      " [ 0.00130922]\n",
      " [-0.01111152]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61575384]\n",
      " [0.05864189]\n",
      " [0.95084182]]\n",
      "layer.weight.data: [[0.61575384]\n",
      " [0.05864189]\n",
      " [0.95084182]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01153379]\n",
      " [-0.00152152]\n",
      " [ 0.01086629]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.62728763]\n",
      " [0.06016341]\n",
      " [0.93997553]]\n",
      "layer.weight.data: [[0.62728763]\n",
      " [0.06016341]\n",
      " [0.93997553]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01061918]\n",
      " [ 0.0013153 ]\n",
      " [-0.01110702]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61666846]\n",
      " [0.05884811]\n",
      " [0.95108255]]\n",
      "layer.weight.data: [[0.61666846]\n",
      " [0.05884811]\n",
      " [0.95108255]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01150953]\n",
      " [-0.00151749]\n",
      " [ 0.01087018]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.62817799]\n",
      " [0.0603656 ]\n",
      " [0.94021238]]\n",
      "layer.weight.data: [[0.62817799]\n",
      " [0.0603656 ]\n",
      " [0.94021238]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01063108]\n",
      " [ 0.0013211 ]\n",
      " [-0.01110267]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.6175469 ]\n",
      " [0.0590445 ]\n",
      " [0.95131505]]\n",
      "layer.weight.data: [[0.6175469 ]\n",
      " [0.0590445 ]\n",
      " [0.95131505]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01148623]\n",
      " [-0.00151365]\n",
      " [ 0.01087393]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.62903313]\n",
      " [0.06055815]\n",
      " [0.94044112]]\n",
      "layer.weight.data: [[0.62903313]\n",
      " [0.06055815]\n",
      " [0.94044112]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01064251]\n",
      " [ 0.00132664]\n",
      " [-0.01109847]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61839062]\n",
      " [0.0592315 ]\n",
      " [0.95153959]]\n",
      "layer.weight.data: [[0.61839062]\n",
      " [0.0592315 ]\n",
      " [0.95153959]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01146385]\n",
      " [-0.00150999]\n",
      " [ 0.01087755]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.62985447]\n",
      " [0.06074149]\n",
      " [0.94066204]]\n",
      "layer.weight.data: [[0.62985447]\n",
      " [0.06074149]\n",
      " [0.94066204]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01065349]\n",
      " [ 0.00133193]\n",
      " [-0.01109441]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61920098]\n",
      " [0.05940956]\n",
      " [0.95175644]]\n",
      "layer.weight.data: [[0.61920098]\n",
      " [0.05940956]\n",
      " [0.95175644]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01144236]\n",
      " [-0.0015065 ]\n",
      " [ 0.01088105]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.63064334]\n",
      " [0.06091607]\n",
      " [0.94087539]]\n",
      "layer.weight.data: [[0.63064334]\n",
      " [0.06091607]\n",
      " [0.94087539]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[ 0.01066403]\n",
      " [ 0.00133699]\n",
      " [-0.01109048]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.61997931]\n",
      " [0.05957908]\n",
      " [0.95196587]]\n",
      "layer.weight.data: [[0.61997931]\n",
      " [0.05957908]\n",
      " [0.95196587]]\n",
      "layer.weight.grad.__mul__(Tensor([self.learning_rate])): Tensor([[-0.01142171]\n",
      " [-0.00150318]\n",
      " [ 0.01088444]], requires_grad=False)\n",
      "after update layer.weight.data: [[0.63140102]\n",
      " [0.06108226]\n",
      " [0.94108144]]\n"
     ]
    }
   ],
   "source": [
    "l = nn.Linear(3, 1, need_bias=False)\n",
    "\n",
    "\"TODO: define an optimizer\"\n",
    "optimizer = optim.SGD([l], learning_rate=0.01)\n",
    "\n",
    "\"TODO: deficne a loss function\"\n",
    "criterion= loss_func.MeanSquaredError\n",
    "\n",
    "\"TODO: print weight and bias of linear layer\"\n",
    "print(\"initial W and bias:\")\n",
    "print(l.weight)\n",
    "print(l.bias)\n",
    "\n",
    "batch_size = 50\n",
    "epLoss = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # zero grad in each epoch\n",
    "    optimizer.zero_grad()\n",
    "    i=0\n",
    "    for start in range(0, 100, batch_size):\n",
    "        # print(f'--------------------------------------------------------------')\n",
    "        # print(f'iteration: {i}')\n",
    "        i+=1\n",
    "        end = start + batch_size\n",
    "\n",
    "        inputs = X[start:end]\n",
    "        outputs = y[start:end]\n",
    "        # outputs.data = outputs.data.reshape(batch_size, 1)\n",
    "\n",
    "        # print(f'inputs: {inputs}')\n",
    "\n",
    "        # TODO: predicted\n",
    "        predicted_result = l.forward(inputs)\n",
    "        # print(f'predicted result:\\n {predicted_result}')\n",
    "        # print(f'actual result:\\n {outputs}')\n",
    "\n",
    "        # TODO: calculate MSE loss\n",
    "        loss= criterion(predicted_result, outputs)\n",
    "        # print(f'loss depends on:\\n {loss.depends_on}')\n",
    "\n",
    "        # TODO: backward\n",
    "        loss.backward()\n",
    "        # loss.backward(Tensor(1.0, requires_grad=True))\n",
    "\n",
    "        # TODO: add loss to epoch_loss\n",
    "        epoch_loss += loss.data\n",
    "        # print(f'epoch_loss: {epoch_loss}')\n",
    "\n",
    "        # TODO: update w and b using optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "    epLoss.append(epoch_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OUT W and bias:\n",
      "Tensor([[0.63140102]\n",
      " [0.06108226]\n",
      " [0.94108144]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "plt.plot(epLoss, label='Error')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOUT W and bias:\")\n",
    "print(l.weight)\n",
    "print(l.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
